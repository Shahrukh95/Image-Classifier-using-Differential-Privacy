# Image Classifier using Differential Privacy
Conventional machine learning models built using standard pipelines are vulnerable to attacks from adversaries aiming to uncover the underlying training data. It has been demonstrated that adversaries can re-engineer an image classifier to reconstruct images of the CIFAR-10 training set. Machine learning models that are trained on sensitive datasets pose a higher privacy risk. Differential Privacy (DP) is a technique for preserving the privacy of individuals in a dataset while allowing meaningful analysis of the data. The idea of the technique is to add random noise to the data in such a way that no inferences can be made about sensitive data. The dataset used in this analysis contains images of smokers and non-smokers. Images of real people are used in both sets. The dataset is publicly available for download at: https://drive.google.com/file/d/19CoKtewBibqszidkH0TMaY0EefQ9eT0d/view?usp=sharing. It contains 1,996 images of smokers and 1,279 for non-smokers. The images were scaled to a square size of 250x250 px. The original images being larger were shrinked and padded with zero pixels.

# Privacy Budget
Differential Privacy provides a formal guarantee of privacy that prevents an adversary to gain information about individual training points from a machine learning model. This privacy guarantee is quantified by the parameters ε and δ, collectively known as the "Privacy Budget". ϵ is a positive value and the value of δ falls within the range of 0 to 1. In essence, ϵ sets an upper limit on the log-likelihood ratio of any output that can be obtained when the algorithm is run on two datasets that differ by one data point. δ is a small probability that restricts the occurrence of rare outputs that exceed this limit. By setting appropriate values in the privacy budget, we can regulate the trade-off between privacy preservation and data utility.

# DP-SGD
Differentially Private Stochastic Gradient Descent (DP-SGD) is the most widely adopted approach for training neural networks with differential privacy. Instead of using the standard mini-batch gradient estimate of SGD, DP-SGD employs a modified version that incorporates privacy safeguards. To achieve this, the gradient of each training example is clipped, restricting its maximum norm. Moreover, Gaussian noise is introduced that is proportional to the clipping norm which is added to the sum of the clipped gradients. This noise effectively masks the impact of any individual example on the overall sum. Throughout training, a privacy accountant is utilized to monitor the cumulative privacy cost incurred by each evaluation of a privatized mini-batch gradient. The parameters associated with privacy such as the privacy cost and noise progressively increase with each observed mini-batch during training. They also decrease as the noise scale is adjusted, ensuring a controlled level of variance in the gradient estimate. As a result, this process establishes a limit on the number of training iterations possible within a fixed privacy budget while maintaining manageable levels of gradient estimate variation.


