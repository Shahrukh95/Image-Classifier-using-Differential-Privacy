# Image Classifier using Differential Privacy
Differential Privacy (DP) is a technique for preserving the privacy of individuals in a dataset while allowing meaningful analysis of the data. The idea of the technique is to add random noise to the data in such a way that no inferences can be made about sensitive data. The dataset used in this analysis contains images of smokers and non-smokers. Images of real people are used in both sets. The dataset is publicly available for download at: https://drive.google.com/file/d/19CoKtewBibqszidkH0TMaY0EefQ9eT0d/view?usp=sharing. It contains 1,996 images of smokers and 1,279 for non-smokers. The images were scaled to a square size of 250x250 px. The original images being larger were shrinked padded with zeros (i.e. SAME padding was used).

# Introduction
Differential Privacy provides a formal guarantee of privacy that prevents an adversary to gain information about individual training points from a machine learning model. Conventional machine learning models built using standard pipelines are vulnerable to attacks from adversaries aiming to uncover the underlying training data. It has been demonstrated that adversaries can re-engineer an image classifier to reconstruct images of the CIFAR-10 training set [1]. Machine learning models that are trained on sensitive datasets pose a higher privacy risk.